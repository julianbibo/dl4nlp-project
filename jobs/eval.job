#!/bin/bash
#SBATCH --nodes=1                           # node count
#SBATCH --ntasks=1                          # total number of tasks across all nodes
#SBATCH --cpus-per-task=1                   # cpu-cores per task (>1 if multi-threaded tasks), 4 is default
#SBATCH --gpus=1                            # number of gpus per node
#SBATCH --partition=gpu_a100	            # partition
#SBATCH --time=00:30:00                     # total run time limit (HH:MM:SS)
#SBATCH --output=logs/%x/%j_%x.log          # output file

############################################################
cd ~/dl4nlp-project  # sanity check, for snellius
source .venv/bin/activate
python -c "import torch; print('Torch version:', torch.__version__); print('CUDA check:', torch.cuda.is_available())"  # check torch version
############################################################

# model options:
# Qwen/Qwen2-7B
# checkpoints/en_de/finetune-lr\=0.001-15042904/checkpoint-2238/
# Qwen/Qwen2-1.5B-Instruct
# Qwen/Qwen2-0.5B-Instruct
# facebook/nllb-200-distilled-600M
# meta-llama/Llama-3.2-3B-Instruct

# Finetuned models:
# Learning rate tuning:
# --model checkpoints/en_de/finetune-lr=0.01-15068133/checkpoint-2517/
# --model checkpoints/en_de/finetune-lr=0.001-15068132/checkpoint-2517/
# --model checkpoints/en_de/finetune-lr=0.0001-15068135/checkpoint-2517/

# LoRA r and alpha = 8, batch size = 8
# --model checkpoints/en_fr/finetune-lr=0.001-15175332/checkpoint-1386/

# checkpoints/en_de/finetune-lr\=0.001-15131437/checkpoint-2517
# --model checkpoints/en_de/finetune-lr=0.001-15172222/checkpoint-2517 \


python src/eval.py \
  --model checkpoints/en_fr/finetune-lr=0.001-15175332/checkpoint-1386/ \
  --data_dir data/wmt22 \
  --split test \
  --source_lang en \
  --target_lang es \
  --batch_size 16 \
  --dtype float16 \
