{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ca2203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import sys, os, copy\n",
    "\n",
    "# be able to import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.models.causal_lm import NMTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a59bd0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0022f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: replace with accurate model directory, preferably not newer than 2023\n",
    "model_dir = \"../data/qwen0-5b\"\n",
    "out_dir = \"../data/out\"\n",
    "\n",
    "# Load model\n",
    "model = NMTModel(model_dir, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5088553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['あなたは何か食べたいですか？\\n', 'Wollen Sie etwas essen?\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt format adapted from \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\"\n",
    "example_prompts = [\n",
    "    \"Translate this from English to Japanese: English: Would you like something to eat? Japanese: \",\n",
    "    \"Translate this from English to German: English: Would you like something to eat? German: \",\n",
    "]\n",
    "\n",
    "model.prompt_batch(example_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98d390",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c0af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657b05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_TOKEN = -100 # TODO: set in loss function\n",
    "\n",
    "def full_lang_name(abbr: str):\n",
    "    \"\"\" \n",
    "    Returns the full name of the language from its abbrieviation.\n",
    "    \"\"\"\n",
    "\n",
    "    NAMES = {\n",
    "        \"es\": \"Spanish\",\n",
    "        \"fr\": \"French\",\n",
    "        \"pt\": \"Portuguese\",\n",
    "        \"de\": \"German\",\n",
    "        \"it\": \"Italian\",\n",
    "        \"ru\": \"Russian\",\n",
    "        \"en\": \"English\",\n",
    "    }\n",
    "\n",
    "    return NAMES[abbr]\n",
    "\n",
    "class Medline(Dataset):\n",
    "    def __init__(self, lang_from: str, lang_to: str, folder: str):\n",
    "        \"\"\"\n",
    "        Loads biomedical dataset from the medline corpus 2022.\n",
    "        Samples will be in the language specified by `lang_from`\n",
    "        and labels in the language `lang_to`. One language must be 'en' (English),\n",
    "        the other one of {'es' (Spanish), 'fr' (French), 'pt' (Portuguese), 'de' (German), 'it' (Italian), 'ru' (Russian)}.\n",
    "\n",
    "        Reads from the directory {folder}/en_{other language} (e.g., wmt22/en_pt). Assumes file names within that directory \n",
    "        follow the following convention: {file id}_{language}.txt (e.g., for file ID 120 we need files 120_en.txt and 120_pt.txt for english to/from portuguese).\n",
    "        \"\"\"\n",
    "\n",
    "        VALID_LANGS = { \"es\", \"fr\", \"pt\", \"de\", \"it\", \"ru\", \"en\" }\n",
    "        \n",
    "        assert lang_from in VALID_LANGS, f\"Specified language '{lang_from}' is not valid! (must be one of {VALID_LANGS})\"\n",
    "        assert lang_to in VALID_LANGS, f\"Specified language '{lang_to}' is not valid! (must be one of {VALID_LANGS})\"\n",
    "        assert lang_from == \"en\" or lang_to == \"en\", \"One of the languages must be english!\"\n",
    "        assert lang_from != lang_to, \"The from and to language may not be the same!\"\n",
    "\n",
    "        # the language that is not english\n",
    "        other_lang = lang_from if lang_from != \"en\" else lang_to\n",
    "\n",
    "        self.data_dir = os.path.join(folder, f\"en_{other_lang}\")\n",
    "\n",
    "        # load file IDs\n",
    "        self.ids = []\n",
    "        for file in os.listdir(self.data_dir):\n",
    "            self.ids.append(file.split(\"_\")[0])\n",
    "\n",
    "        self.lang_from = lang_from\n",
    "        self.lang_to = lang_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def read_file(self, idx, lang) -> str:\n",
    "        \"\"\"\n",
    "        Tries to read the contents of a language file. \n",
    "        # Errors\n",
    "        * If the file corresponding to `idx` and `lang` does not exist.\n",
    "        \"\"\"\n",
    "\n",
    "        path = os.path.join(self.data_dir, f\"{idx}_{lang}.txt\")\n",
    "\n",
    "        with open(path, \"r\") as file:\n",
    "            return file.read().rstrip()\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Fetches a sample and target.\n",
    "        \"\"\"\n",
    "\n",
    "        idx = self.ids[index]\n",
    "\n",
    "        source = self.read_file(idx, self.lang_from)\n",
    "        target = self.read_file(idx, self.lang_to)\n",
    "\n",
    "        return source, target\n",
    "    \n",
    "dset = Medline(\"de\", \"en\", \"../data/wmt22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_prompt_skeleton(lang_from: str, lang_to: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a translation prompt skeleton with the source yet to be plugged in.\n",
    "    The format is adapted from \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\".\n",
    "    \"\"\"\n",
    "    return f\"Translate this from {lang_from} to {lang_to}: {lang_from}: {{}} {lang_to}: \"\n",
    "\n",
    "def collate_fn(batch, tokenizer, prompt_form: str):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing input_ids, labels, and attention_mask entries \n",
    "    by taking a batch of string source target pairs, formatting them into a translation prompt,\n",
    "    and tokenizing.\n",
    "    \"\"\"  \n",
    "\n",
    "    source = [prompt_form.format(s[0]) for s in batch]\n",
    "    target = [s[1] for s in batch]\n",
    "\n",
    "    source_toks_raw = tokenizer(source, add_special_tokens=False)[\"input_ids\"]\n",
    "    target_toks_raw = tokenizer(target, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # build tokens\n",
    "    bos = [tokenizer.bos_token_id] if tokenizer.bos_token_id is not None else []\n",
    "    eos = [tokenizer.eos_token_id]\n",
    "\n",
    "    source_toks = [bos + s + t + eos for s, t in zip(source_toks_raw, target_toks_raw)]\n",
    "    toks = tokenizer.pad({ \"input_ids\": source_toks }, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # for labels, set prompt tokens to IGNORE_TOKEN as we don't want to compute loss over those\n",
    "    toks[\"labels\"] = copy.deepcopy(toks[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(toks[\"labels\"])):\n",
    "        toks[\"labels\"][i][:len(source_toks_raw[i])] = IGNORE_TOKEN\n",
    "    return toks\n",
    "    \n",
    "\n",
    "prompt_form = get_translation_prompt_skeleton(full_lang_name(dset.lang_from), full_lang_name(dset.lang_to))\n",
    "\n",
    "dloader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=16,\n",
    "    collate_fn=partial(collate_fn, tokenizer=model.tokenizer, prompt_form=prompt_form)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70d20c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643]])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b40b",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f40e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# load finetuneable model\n",
    "# TODO: find good parameters\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model_peft = get_peft_model(model.model, peft_config)\n",
    "\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find good parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# TODO: find good parameters\n",
    "# TODO: override get_train_dataloader, get_eval_dataloader, and get_test_dataloader\n",
    "# TODO: set ignore_index in the loss function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=...,\n",
    "    eval_dataset=...,\n",
    "    processing_class=model.tokenizer,\n",
    "    data_collator=...,\n",
    "    compute_metrics=...,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
