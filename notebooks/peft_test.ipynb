{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca2203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur1836/dl4nlp-project/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import sys, os, copy\n",
    "from functools import partial\n",
    "\n",
    "# be able to import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.models.causal_lm import NMTModel\n",
    "from src.data import Medline, collate_translations, get_translation_prompt_skeleton, full_lang_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a59bd0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0022f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: replace with accurate model directory, preferably not newer than 2023\n",
    "# model_dir = \"../data/qwen0-5b\"\n",
    "# out_dir = \"../data/out\"\n",
    "\n",
    "# # load model\n",
    "# model = NMTModel(model_dir, \"cpu\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map='cpu',\n",
    "    dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5088553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format adapted from \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\"\n",
    "# example_prompts = [\n",
    "#     \"Translate this from English to Japanese: English: Would you like something to eat? Japanese: \",\n",
    "#     \"Translate this from English to German: English: Would you like something to eat? German: \",\n",
    "# ]\n",
    "\n",
    "# model.prompt_batch(example_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98d390",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657b05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Medline(\"de\", \"en\", \"../data/wmt22\")\n",
    "\n",
    "train_dset, test_dset = random_split(dset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a71fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_form \u001b[38;5;241m=\u001b[39m get_translation_prompt_skeleton(full_lang_name(dset\u001b[38;5;241m.\u001b[39mlang_from), full_lang_name(dset\u001b[38;5;241m.\u001b[39mlang_to))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_memory_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m dloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      6\u001b[0m     dset,\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      8\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mpartial(collate_translations, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, prompt_form\u001b[38;5;241m=\u001b[39mprompt_form, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39m_dump_snapshot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter_dataloader.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dl4nlp-project/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:936\u001b[0m, in \u001b[0;36m_record_memory_history\u001b[0;34m(enabled, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _record_memory_history_legacy(enabled, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_record_memory_history_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl4nlp-project/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:949\u001b[0m, in \u001b[0;36m_record_memory_history_impl\u001b[0;34m(enabled, context, stacks, max_entries, device, clear_history, compile_context, global_record_annotations)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_record_memory_history_impl\u001b[39m(\n\u001b[1;32m    940\u001b[0m     enabled: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    941\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     global_record_annotations: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    948\u001b[0m ):\n\u001b[0;32m--> 949\u001b[0m     \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_record_memory_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_record_annotations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "prompt_form = get_translation_prompt_skeleton(full_lang_name(dset.lang_from), full_lang_name(dset.lang_to))\n",
    "\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "dloader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=32,\n",
    "    collate_fn=partial(collate_translations, tokenizer=tokenizer, prompt_form=prompt_form, device=\"cpu\")\n",
    ")\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"after_dataloader.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d20c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 1284])\n",
      "attention_mask torch.Size([32, 1284])\n",
      "labels torch.Size([32, 1284])\n"
     ]
    }
   ],
   "source": [
    "next(iter(dloader))\n",
    "\n",
    "# get shape of a batch\n",
    "batch = next(iter(dloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b40b",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f40e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# load finetuneable model\n",
    "# TODO: find good parameters\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model_peft = get_peft_model(model, peft_config)\n",
    "\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e44e634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: find good parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m----> 3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[43mout_dir\u001b[49m,\n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      6\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      8\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m      9\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# TODO: find good parameters\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO: override get_train_dataloader, get_eval_dataloader, and get_test_dataloader\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# TODO: set ignore_index in the loss function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EncoderTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: find good parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# TODO: find good parameters\n",
    "# TODO: override get_train_dataloader, get_eval_dataloader, and get_test_dataloader\n",
    "# TODO: set ignore_index in the loss function\n",
    "trainer = EncoderTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dset,\n",
    "    eval_dataset=test_dset,\n",
    "    processing_class=model.tokenizer,\n",
    "    compute_metrics=...,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445165d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
