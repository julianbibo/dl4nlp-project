{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca2203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import sys, os, copy\n",
    "from functools import partial\n",
    "\n",
    "# be able to import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.models.causal_lm import NMTModel\n",
    "from src.data import Medline, collate_translations, get_translation_prompt_skeleton, full_lang_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a59bd0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0022f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: replace with accurate model directory, preferably not newer than 2023\n",
    "model_dir = \"../data/qwen0-5b\"\n",
    "out_dir = \"../data/out\"\n",
    "\n",
    "# load model\n",
    "model = NMTModel(model_dir, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5088553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['あなたは何か食べたいですか？\\n', 'Wollen Sie etwas essen?\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt format adapted from \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\"\n",
    "example_prompts = [\n",
    "    \"Translate this from English to Japanese: English: Would you like something to eat? Japanese: \",\n",
    "    \"Translate this from English to German: English: Would you like something to eat? German: \",\n",
    "]\n",
    "\n",
    "model.prompt_batch(example_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98d390",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657b05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Medline(\"de\", \"en\", \"../data/wmt22\")\n",
    "\n",
    "train_dset, test_dset = random_split(dset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_form = get_translation_prompt_skeleton(full_lang_name(dset.lang_from), full_lang_name(dset.lang_to))\n",
    "\n",
    "dloader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=16,\n",
    "    collate_fn=partial(collate_translations, tokenizer=model.tokenizer, prompt_form=prompt_form)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d20c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b40b",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f40e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# load finetuneable model\n",
    "# TODO: find good parameters\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model_peft = get_peft_model(model.model, peft_config)\n",
    "\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find good parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# TODO: find good parameters\n",
    "# TODO: override get_train_dataloader, get_eval_dataloader, and get_test_dataloader\n",
    "# TODO: set ignore_index in the loss function\n",
    "trainer = EncoderTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dset,\n",
    "    eval_dataset=test_dset,\n",
    "    processing_class=model.tokenizer,\n",
    "    compute_metrics=...,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
