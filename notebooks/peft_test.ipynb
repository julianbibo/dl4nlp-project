{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca2203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/dl4nlp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import sys, os, copy\n",
    "from functools import partial\n",
    "\n",
    "# be able to import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.models.causal_lm import NMTModel\n",
    "from src.data import Medline, collate_translations, get_translation_prompt_skeleton, full_lang_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a59bd0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0022f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: replace with accurate model directory, preferably not newer than 2023\n",
    "model_dir = \"../data/qwen0-5b\"\n",
    "out_dir = \"../data/out\"\n",
    "\n",
    "# load model\n",
    "model = NMTModel(model_dir, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5088553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['あなたは何か食べたいですか？\\n', 'Wollen Sie etwas essen?\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt format adapted from \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\"\n",
    "example_prompts = [\n",
    "    \"Translate this from English to Japanese: English: Would you like something to eat? Japanese: \",\n",
    "    \"Translate this from English to German: English: Would you like something to eat? German: \",\n",
    "]\n",
    "\n",
    "model.prompt_batch(example_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98d390",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657b05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Medline(\"de\", \"en\", \"../data/wmt22\")\n",
    "\n",
    "train_dset, test_dset = random_split(dset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_form = get_translation_prompt_skeleton(full_lang_name(dset.lang_from), full_lang_name(dset.lang_to))\n",
    "\n",
    "dloader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=16,\n",
    "    collate_fn=partial(collate_translations, tokenizer=model.tokenizer, prompt_form=prompt_form)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d20c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643],\n",
       "        [ 27473,    419,    504,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        ...,\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643],\n",
       "        [  -100,   -100,   -100,  ..., 151643, 151643, 151643]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b40b",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f40e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# load finetuneable model\n",
    "# TODO: find good parameters\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model_peft = get_peft_model(model.model, peft_config)\n",
    "\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb6129c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer for optimizing encoder-only NMT transformer models.\n",
    "\n",
    "    During training and evaluation, it provides source sentences (possibly wrapped in a prompt to indicate the type of task to the model)\n",
    "    and optimizes the model for predicting correct target sentences. In other words, loss or metrics are not calculated over the\n",
    "    source prompt, only over the target sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompt_skeleton: str, tokenizer, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.collate_fn = partial(collate_translations, tokenizer=tokenizer, prompt_form=prompt_skeleton)\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader( # TODO: use accelerator.prepare?\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset = None):\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.eval_dataset[eval_dataset] if isinstance(eval_dataset, str) else eval_dataset,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        return DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=self.collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe37067",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    learning_rate=1e-3, # TODO: finetune\n",
    "    # TODO: make as big as GPU permits\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # prevent source labels to be included in loss\n",
    "    label_smoothing_factor=0.0,\n",
    ")\n",
    "\n",
    "# TODO: set ignore_index in the loss function\n",
    "trainer = EncoderTrainer(\n",
    "    # prompt used for aiding the model to make translations\n",
    "    prompt_skeleton=get_translation_prompt_skeleton(full_lang_name(dset.lang_from), full_lang_name(dset.lang_to)),\n",
    "    tokenizer=model.tokenizer,\n",
    "\n",
    "    model=model.model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dset,\n",
    "    eval_dataset=test_dset,\n",
    "    processing_class=model.tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
